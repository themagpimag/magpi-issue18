Hardware/Software co-design with Logi-pi


Outline :




        What is hardware software co-design ?


Co-design consists of designing a project as a mixture of software components and hardware components. Software components usually run on processors (CPU, DSP, GPU ...) whereas hardware components run on an FPGA (Field Programmable Gate Array) or a dedicated ASIC (Application Specific Integrated Circuit). This kind of design method is used to take advantage of the inherent parallelism between the tasks of the application and ease re-use between applications.
 
Such co-designs requires the user to :


- partition its application into the hardware and software components
- map the components to the resources
- schedule the software components
- manage the communications between the different components


These steps can either be performed by co-design tools, or by hand, based on the knowledge of the designer. In the following we will describe how the Logi-pi can be used to perform such a co-design to run real-time control oriented applications or high performance applications with the raspberry pi.


        Communication between the Logi-pi and raspberry-pi


One key aspect of a co-design is the ability to communicate between the processing units of the platform.  The processing units in this case are the FPGA and the Raspberry Pi.  In the logi project we chose to use the wishbone bus to ensure fast and reliable communication between the hardware and software components. The raspberry pi does not provide a wishbone bus on its expansion, so we decided to take advantage of the SPI port and to design a hardware “wrapper” component  in the FPGA that transforms this serial bus into a 16 bit wishbone master component. The use of this bus allows us to take advantage of the extensive repository of open-source HDL components hosted on open-cores.org.
To handle the communication on the SPI bus side each transaction is composed of the following information.


1) set slave select low 
1) send a 16 bit command word with the 14th msb bits being the address of the access the 2nd bit to indicate burst mode (1) , and 1st bit to indicate read (1) or write (0).
2) send/receive 16 bit words to/from the address set in the first transaction. If burst mode is set, the address will be increased on each subsequent access until the chip select line is set to high (end of transaction). 
3) set slave select high



Such transactions allow us to take advantage of the integrated SPI controller of the raspberry-pi which uses a 4096 byte fifo. Because we run the bus at 32Mhz which gives a theoretical bandwidth of 4MB/s, this means that we can get the following useful bandwidth with our interface:


1) For a single 16 bit access :(16 bit data)/(16 bit command + 16 bit data)*4MB/s => 2MB/s
2) For a 4094 byte access : ( 2047 * (16 bit data))/(16 bit command + 2047 * (16 bit data)) * 4MB/s  =>  3,99 MB/s


This means that for most control based applications (writing/reading register), we get half of the theoretical bandwidth, but for data based applications (writing/reading fifo/memory) we get 99% of the theoretical bandwidth. One can argue that getting rid of the wishbone interface with application specific data communication (formatted packet of data) on the spi bus could get us the max bandwidth, but this would break the opencores.org support and will also break the generic approach we propose. This communication is abstracted using a dedicated C API that provides memory read/write functions and also fifo management for communications based on fifo embedded into the FPGA. We also provide a library of hardware components (VHDL) that the user can integrate into designs (servo controller, pwm controller, fifo controller, pid controller …).


 
        Abstracting the communication layer with python


Because the raspberry-pi platform is targeted toward education, we decided to abstract the communication over the spi bus using a Python library that provides easy access to the Logi-pi platform. This library can then be used to abstract a complete hardware design by a Python class whose methods are directly linked to the hardware components. This is achieved by writing a python class that reflecst the hardware component inside the FPGA. This class provides access to the peripheral registers or higher level functions. For example,  a servo controller embedded inside the FPGA,  we can design a python class with the setPulse, setAngle method. These methods will then communicate with the hardware using the Logi-pi library and write the appropriate values to the corresponding peripheral address. This allows us to write the software side of the co-design in python and take advantage of the language features and libraries with high performance side being implemented in the FPGA.




        A basic example of hardware/software co-design with the logi-face


The logi-face is a demo based on the Logi-pi platform that acts as a telepresence device. This demo uses a set of actuators (servos, leds) to display emotions on an animatronic face.  This application is composed of a set of tasks pictured in the following diagram . 







The tasks are partitioned on the Logi-pi platform with software components running on the raspberry-pi and hardware components on the Logi-pi. The choice of software components was made to take advantage of existing software libraries such as espeak text to speech engine and linphone SIP client. The hardware component are time critical tasks such as servo driver, led matrix controller, ADC controller and PWM controller. Further work could lead to optimize parts of the espeak tts engine in hardware (FPGA). 


All the missing software bits (emotion generation from sip clients chat session) were written in python to leverage the language features and ease development. The hardware side was abstracted using a dedicated python class to abstract the peripherals inside the FPGA. 







        Video application overview using the logipi [a]




Hardware/Software co-design tends to be relevant to high performance and time-critical applications. Image processing is the kind of application that requires high throughput, and frame accurate synchronization to provide relevant results for higher level filtering application (feature tracking, visual odometry …). In the following example we developed a canny edge detector leveraging the processing power of the FPGA and the flexibility of the raspberry pi for the encoding and networking tasks.  



The partitioning leads to the following implementation :



The hardware components for the FPGA side are extracted from a project I initiated for hardware computer vision components (https://github.com/jpiat/hard-cv). In this application a communication bottleneck on the SPI bus forces us to send a downscaled image to the raspberry-pi to meet the framerate constraints. All the image processing is performed on QVGA (320x240) frames at 30 FPS (hardware can run flawlessly on larger format and higher frame-rate) and the result is downscaled to QQVGA (160x120). A custom mjpeg-streamer plugin reads a frame on the raspberry pi side, encodes it to jpeg using libjpeg and passes the frame to mjpeg-streamer for streaming (see video : http://www.youtube.com/watch?v=0HwEcmF5wxw). 


 
Conclusion


The logipi is a nice platform to develop co-designed applications at low cost and in a small package. The ARM11 processor has plenty of computing power while the spartan 6 LX9 FPGA provides enough logic for signal processing applications. This kind of platform can easily be integrated in an electrical engineering courses because the student can first learn software development with the raspberry-pi (in python for example), then use the pre-developed hardware architecture on the FPGA to learn parallelism and then learn hardware development with the Logi-pi. For example the video application could be taught in the following steps :
1) Code a line detection algorithm using the OpenCV library in python
2) Take the code of the last step and replace the opencv calls by calls to the hardware (with the FPGA implementing canny). Learn how to re-schedule the code to take advantage of the available parallelism.
3) Code a small part of the all Canny pipeline in hardware and use the developed architecture in the application.
4) Use the output of the architecture to drive a line following robot.


All the code of the application found in this article can be downloaded from https://github.com/fpga-logi/Logi-projects. 
[a]Michael Jones:
(If you like you can send me some case scenarios that you would like recorded and I will record and send to you so you can voice over.   Or I can do the voice as well.  You are the master of logi-face)